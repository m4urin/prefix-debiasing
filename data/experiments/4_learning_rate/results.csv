model_name,model_type,training_time (minutes),final_loss,n_parameters,seat_6_effect_size,seat_6_p_val,seat_7_effect_size,seat_7_p_val,seat_8_effect_size,seat_8_p_val,lpbs_adjectives_bias_score,lpbs_adjectives_bias_score_std,lpbs_kaneko_stereotypes_bias_score,lpbs_kaneko_stereotypes_bias_score_std,lpbs_occupations_bias_score,lpbs_occupations_bias_score_std,loss_function,epochs,batch_size,lr,num_warmup_steps,seed,prefix_mode,prefix_layers,n_prefix_tokens
roberta-base,base,0.0,inf,0,1.0989412127489384,1e-05,0.8279529656397387,1e-05,0.49038101873841317,0.00352,0.8664145469665527,0.6923962831497192,1.0164146423339844,0.7245630025863647,0.8532798886299133,0.7991033792495728,,,,,,,,,
roberta-base,finetune,21.3,28.82493782043457,124697433,-0.026532614645265296,0.56116,0.0985626376426753,0.27901,0.02082471960624849,0.4581,0.3015301823616028,0.5511841177940369,0.7584254741668701,0.9279823899269104,0.7492287755012512,0.573536217212677,kaneko,3.0,32.0,4e-05,200.0,42.0,,,
roberta-base,prefix,18.2,144.48519897460938,147456,-0.10972277255242958,0.7299,-0.30785133075871735,0.96779,-0.4947463054465664,0.99582,0.039192140102386475,0.03705595061182976,0.04482429474592209,0.03379911929368973,0.04110260307788849,0.0351393036544323,kaneko,3.0,32.0,0.01,200.0,42.0,replace,all,16.0
