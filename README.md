# Debiasing through Prefix Tuning

[Temporary paper](https://github.com/m4urin/prefix-debiasing/blob/main/paper.pdf).

Using Prefix-tuning as a means for debiasing language models, such as BERT. It achieves results on par with traditional finetuning approaches, while requiring significantly less storage space by only keeping a tiny fraction of the original model's parameters. This makes prefix-tuning an attractive option for those seeking to mitigate biases in their language models while maximizing resource efficiency.
